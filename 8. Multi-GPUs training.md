1. 多GPU训练一般包括了几个DataParallel, DistributdDataParallel, Apex的半精度训练等等。
2. 一般Dataparallel 对于代码变动最小，但是会带来GPU 0 的显存占用会过多，这时就推荐使用BalancedDataParallel，这个部分是从transformer-xl借来的。[link](https://github.com/kimiyoung/transformer-xl)
3. DistributedDataParallel是启动多进程来做的，如果中间想要做测试或者其他一些单进程的工作会比较麻烦。
4. Apex可以加两种DataParallel都能够有效减小模型的大小。
